{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Inference_and_Evaluation_of_BERT_pair_NLI_B.ipynb","provenance":[],"collapsed_sections":["LACVADTH_U8N","qG4vUkDMGtZ6","Bapw7CmTT38f","2m6lnQaklouY"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"hIANK2Rn_AS2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653638771113,"user_tz":-420,"elapsed":14079,"user":{"displayName":"project colab","userId":"14559293068416285175"}},"outputId":"54acdae7-1200-4c6f-d844-51709687bb98"},"source":["# Install dependencies\n","!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n","\u001b[K     |████████████████████████████████| 4.2 MB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 2.9 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 61.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 51.7 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"]}]},{"cell_type":"code","metadata":{"id":"0uIEvh0A_FUk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653638790688,"user_tz":-420,"elapsed":19595,"user":{"displayName":"project colab","userId":"14559293068416285175"}},"outputId":"49309cc8-d1a6-4908-ee13-87819fdf888f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/PIL/BERT-ABSA/Bert-pair/NLI-B"],"metadata":{"id":"m-AFKNAok6xk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653638790688,"user_tz":-420,"elapsed":25,"user":{"displayName":"project colab","userId":"14559293068416285175"}},"outputId":"3ce49ada-d11b-4502-e68c-fdf33cf71ace"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/PIL/BERT-ABSA/Bert-pair/NLI-B\n"]}]},{"cell_type":"code","metadata":{"id":"GK_-dM26_TGi","executionInfo":{"status":"ok","timestamp":1653638797400,"user_tz":-420,"elapsed":6723,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["import json\n","import torch\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np\n","import transformers\n","import torch.nn.functional as F\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score, accuracy_score\n","from transformers import BertModel, BertTokenizer\n","\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"VGTic4EP_XSG","executionInfo":{"status":"ok","timestamp":1653638798437,"user_tz":-420,"elapsed":9,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["class SentimentClassifier(nn.Module):\n","  \"\"\"\n","  This class defines the model architecture which is simply a fully-connected\n","  layer on top of a pre-trained BERT model. \n","  \"\"\"\n","\n","  def __init__(self, BERT_MODEL):\n","    super(SentimentClassifier, self).__init__()\n","    self.bert = BertModel.from_pretrained(BERT_MODEL)\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(self.bert.config.hidden_size, 2) # Binary Classifier\n","\n","  def forward(self, ids, mask, token_type_ids):\n","    last_hidden_state, pooled_output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n","    output = self.drop(pooled_output)\n","    return self.out(output)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LACVADTH_U8N"},"source":["# Inference on BERT-pair NLI-B"]},{"cell_type":"code","metadata":{"id":"hDO650jp_Z7X","executionInfo":{"status":"ok","timestamp":1653638798438,"user_tz":-420,"elapsed":8,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["class SentiHood:\n","  \"\"\"\n","  This class tokenizes the input text using the pre-trained BERT tokenizer \n","  (wordpiece) and returns the corresponding tensors.\n","  \"\"\"\n","  \n","  def __init__(self, opinions_id, text, auxiliary_sentence, targets, tokenizer, max_len):\n","    self.opinions_id = opinions_id\n","    self.text = text\n","    self.auxiliary_sentence = auxiliary_sentence\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","    self.targets = targets\n","\n","  def __len__(self):\n","    return len(self.targets)\n","\n","  def __getitem__(self, item):\n","    opinions_id = self.opinions_id[item]\n","    text = str(self.text[item])\n","    auxiliary_sentence = str(self.auxiliary_sentence[item])\n","    targets = self.targets[item]\n","\n","    text = text + ' ' + auxiliary_sentence\n","\n","    inputs = self.tokenizer.encode_plus(\n","        text,\n","        add_special_tokens = True,\n","        max_length = self.max_len,\n","        pad_to_max_length = True\n","    )\n","\n","    ids = inputs[\"input_ids\"]\n","    mask = inputs[\"attention_mask\"]\n","    token_type_ids = inputs[\"token_type_ids\"]\n","\n","    return {\n","        \"ids\": torch.tensor(ids, dtype=torch.long),\n","        \"mask\": torch.tensor(mask, dtype=torch.long),\n","        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n","        \"targets\": torch.tensor(targets, dtype=torch.long),\n","        \"opinions_id\": torch.tensor(opinions_id, dtype=torch.long)\n","    }"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"tc_tUUJABEVB","executionInfo":{"status":"ok","timestamp":1653638798993,"user_tz":-420,"elapsed":561,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["def infer_loop_function(data_loader, model, device):\n","  \"\"\"\n","  This function performs the inference on testing sets and stores the predicted\n","  values.\n","  \"\"\"\n","\n","  model.eval()\n","\n","  df_pred = pd.DataFrame({\"id\": [], \"probability\": [], \"actual\": []})\n","\n","  ii = 0\n","  for bi, d in tqdm(enumerate(data_loader), total=len(data_loader), ncols=80):\n","    opinions_id = d[\"opinions_id\"]\n","    ids = d[\"ids\"]\n","    mask = d[\"mask\"]\n","    token_type_ids = d[\"token_type_ids\"]\n","    targets = d[\"targets\"]\n","\n","    opinions_id = opinions_id.to(device, dtype=torch.long)\n","    ids = ids.to(device, dtype=torch.long)\n","    mask = mask.to(device, dtype=torch.long)\n","    token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","    targets = targets.to(device, dtype=torch.long)\n","\n","    outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n","    outputs = F.softmax(outputs, dim=-1)\n","\n","    outputs = outputs.detach().cpu().numpy()\n","    targets = targets.detach().cpu().numpy()\n","    opinions_id = opinions_id.detach().cpu().numpy()\n","\n","    # Storing the probability of \"yes\" as the matching score.\n","    for k in range(len(outputs)):\n","      df_pred.loc[ii] = [str(opinions_id[k]), outputs[k][1], targets[k]]\n","      ii += 1\n","\n","  df_pred.to_csv('/content/drive/MyDrive/PIL/BERT-ABSA/Bert-pair/NLI-B/PredictedValues.csv', index=False)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xalt3-vB_doq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c21d7fa3-8148-449e-f4ad-d52dd0995ff5","executionInfo":{"status":"ok","timestamp":1653639707431,"user_tz":-420,"elapsed":839611,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["def run():\n","  \"\"\"\n","  This function defines the necessary hyperparameters and models. It also \n","  loads and tokenizes the testing dataset and execute the inference procedure.\n","  \"\"\"\n","\n","  TRAIN_MAX_LEN = 160\n","  TRAIN_BATCH_SIZE = 24\n","  BERT_MODEL = 'bert-base-uncased'\n","\n","  testing_set_path = '/content/drive/MyDrive/PIL/BERT-ABSA/Bert-pair/NLI-B/Datasets/testing_set.csv'\n","\n","  df_test = pd.read_csv(testing_set_path)\n","  df_test = df_test.reset_index(drop=True)\n","\n","  tokenizer = transformers.BertTokenizer.from_pretrained(BERT_MODEL)\n","\n","  test_dataset = SentiHood(\n","      opinions_id = df_test['id'].values,\n","      text = df_test['text'].values,\n","      auxiliary_sentence = df_test['auxiliary_sentence'],\n","      targets = df_test['sentiment'].values,\n","      tokenizer = tokenizer,\n","      max_len = TRAIN_MAX_LEN\n","  )\n","  print(f\"Testing Set: {len(test_dataset)}\")\n","\n","  test_data_loader = torch.utils.data.DataLoader(\n","      test_dataset,\n","      batch_size = TRAIN_BATCH_SIZE,\n","      shuffle=False\n","  )\n","\n","  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","  print(f\"Device: {device}\")\n","\n","  model = torch.load('/content/drive/MyDrive/PIL/BERT-ABSA/Bert-pair/NLI-B/Models/Models3.bin')\n","  infer_loop_function(data_loader=test_data_loader, model=model, device=device)\n","      \n","if __name__ == \"__main__\":\n","  run()"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing Set: 67644\n","Device: cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["  0%|                                                  | 0/2819 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","100%|███████████████████████████████████████| 2819/2819 [13:38<00:00,  3.44it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"qG4vUkDMGtZ6"},"source":["# Evaluation of BERT-pair NLI-B"]},{"cell_type":"code","metadata":{"id":"5XAgZ22Jav1E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653639762360,"user_tz":-420,"elapsed":55006,"user":{"displayName":"project colab","userId":"14559293068416285175"}},"outputId":"5ff5c15a-79dc-49a1-8a30-705125266e8f"},"source":["\"\"\"\n","Select the sequence with highest matching score, to compute the sentiment.\n","\"\"\"\n","\n","df_NLIB = pd.read_csv('/content/drive/MyDrive/PIL/BERT-ABSA/Bert-pair/NLI-B/PredictedValues.csv')\n","df_NLIM = pd.read_csv('/content/drive/MyDrive/PIL/BERT-ABSA/Bert-pair/NLI-M/PredictedValues.csv')\n","\n","df = pd.DataFrame({\"id\": [], \"predicted\": []})\n","\n","ii = 0\n","for k in tqdm(range(0, df_NLIB.shape[0]-3, 3), ncols=80):\n","  positive_probability = df_NLIB.iloc[k]['probability']\n","  negative_probability = df_NLIB.iloc[k+1]['probability']\n","  none_probability = df_NLIB.iloc[k+2]['probability']\n","\n","  if positive_probability > negative_probability and positive_probability > none_probability:\n","    df.loc[ii] = [str(df_NLIB.iloc[k]['id']), 0]\n","\n","  elif negative_probability > positive_probability and negative_probability > none_probability:\n","    df.loc[ii] = [str(df_NLIB.iloc[k]['id']), 1]\n","\n","  else:\n","    df.loc[ii] = [str(df_NLIB.iloc[k]['id']), 2]\n","  \n","  ii += 1\n","\n","df['predicted'] = df['predicted'].astype(int)\n","df['actual'] = df_NLIM['actual']"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████| 22547/22547 [00:55<00:00, 407.73it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"HtYO_f4pGyJY","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3710d0b4-5b35-46dd-d181-280ec8cbdb50","executionInfo":{"status":"ok","timestamp":1653641261236,"user_tz":-420,"elapsed":309,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["def compute_sentiment_accuracy(df):\n","  \"\"\"This function computes the sentiment classfication accuracy\"\"\"\n","  \n","  accuracy = df[df['predicted'] == df['actual']].shape[0]/df.shape[0] * 100\n","  return round(accuracy, 2)\n","\n","print(f'Sentiment Accuracy of BERT-pair NLI-B = {compute_sentiment_accuracy(df)}%')"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentiment Accuracy of BERT-pair NLI-B = 96.65%\n"]}]},{"cell_type":"code","metadata":{"id":"TzIfR5nxKbv9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e24c3d35-b2a4-44bc-af8a-0dc1df89178d","executionInfo":{"status":"ok","timestamp":1653641268608,"user_tz":-420,"elapsed":2086,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["def compute_aspect_f1_score(df):\n","  \"\"\"\n","  This function computest the macro F1 score of predicted aspects.\n","  0 => Represents that the aspect has not been detected.\n","  1 => Represents that the aspect has been detected.\n","  \"\"\"\n","  \n","  df = df.replace([0, 1], 1).replace(2, 0)\n","\n","  total_f1_score = 0\n","  total = 0\n","  \n","  for i in range(0, df.shape[0], 12):\n","    true_values = df.iloc[i:i+12]['predicted']\n","    predicted_values = df.iloc[i:i+12]['actual']\n","\n","    total_f1_score += f1_score(true_values, predicted_values, average=\"macro\")\n","    total += 1\n","\n","  score = float(total_f1_score)/float(total)*100\n","  return round(score, 2)\n","\n","print(f\"Aspect F1 score: {compute_aspect_f1_score(df)}\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Aspect F1 score: 89.17\n"]}]},{"cell_type":"code","metadata":{"id":"icSi6UoPHLAG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c5304e87-9918-4281-c49a-0f2d7ac2f93e","executionInfo":{"status":"ok","timestamp":1653641272699,"user_tz":-420,"elapsed":871,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["def compute_aspect_accuracy(df):\n","  \"\"\"\n","  This function computes the strict aspect accuracy.\n","  0 => Represents that the aspect has not been detected.\n","  1 => Represents that the aspect has been detected.\n","  \"\"\"\n","  \n","  df = df.replace([0, 1], 1).replace(2, 0)\n","\n","  count = 0\n","  total = 0\n","\n","  for i in range(0, df.shape[0], 12):\n","    true_values = df.iloc[i:i+12]['predicted']\n","    predicted_values = df.iloc[i:i+12]['actual']\n","\n","    if (true_values == predicted_values).all():\n","      count += 1\n","    total += 1\n","\n","  accuracy = float(count)/float(total)*100\n","  return round(accuracy, 2)\n","\n","print(f'Aspect Accuracy (strict) of BERT-pair NLI-B = {compute_aspect_accuracy(df)}%')"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Aspect Accuracy (strict) of BERT-pair NLI-B = 69.77%\n"]}]},{"cell_type":"markdown","metadata":{"id":"Bapw7CmTT38f"},"source":["# Prediction Result Analysis\n","\n","This section analyses the predicted results to find the aspects and sentiments that are most and least accurate.\n","\n","*Note*: Utilizing the fact that first 1491x12 entries in the **above** constructed `df` are related to `location-1` and rest are related to `location-2`. "]},{"cell_type":"code","metadata":{"id":"R090gFrsq3lO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653641288245,"user_tz":-420,"elapsed":9306,"user":{"displayName":"project colab","userId":"14559293068416285175"}},"outputId":"0b5bcf4d-eadb-4934-bf6d-6abc0b525dc9"},"source":["\"\"\"\n","Computes the positive correct, positive total, negative correct, negative total, \n","none correct, none total corresponding to all the aspects of LOCATION1.\n","\"\"\"\n","\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety','shopping', 'touristy', 'transit-location']\n","location1_aspects_result_analysis = {}\n","\n","for i in range(12):\n","  location1_aspects_result_analysis[aspects[i]] = [[0 ,0], [0 ,0], [0 ,0]]\n","\n","for i in tqdm(range(0, df['id'].unique().shape[0]*12-12, 12), ncols=80):\n","  for j in range(12):\n","    if df.loc[i+j]['actual'] == df.loc[i+j]['predicted']:\n","      location1_aspects_result_analysis[aspects[j]][int(df.loc[i+j]['actual'])][0] += 1\n","    \n","    location1_aspects_result_analysis[aspects[j]][int(df.loc[i+j]['actual'])][1] += 1"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████████████████████████████████| 1490/1490 [00:09<00:00, 161.22it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"v10o29bn6ghR","executionInfo":{"status":"ok","timestamp":1653641288626,"user_tz":-420,"elapsed":5,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["df_location_aspect = pd.DataFrame({\"location\": [], \"aspect\": [], \"positive correct\": [], \"positive total\": [], \"negative correct\": [], \"negative total\": [], \"none correct\": [], \"none total\": [],})\n","\n","ii = 0\n","for key in location1_aspects_result_analysis.keys():\n","  df_location_aspect.loc[ii] = ['LOCATION1', f\"{key}\", \n","                                location1_aspects_result_analysis[key][0][0], \n","                                location1_aspects_result_analysis[key][0][1], \n","                                location1_aspects_result_analysis[key][1][0], \n","                                location1_aspects_result_analysis[key][1][1], \n","                                location1_aspects_result_analysis[key][2][0], \n","                                location1_aspects_result_analysis[key][2][1]]\n","  ii += 1"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEt5ghsD46Sp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653641290776,"user_tz":-420,"elapsed":2154,"user":{"displayName":"project colab","userId":"14559293068416285175"}},"outputId":"bd119ff7-eed8-4985-ca66-1d93a74b3fbe"},"source":["\"\"\"\n","Computes the positive correct, positive total, negative correct, negative total, \n","none correct, none total corresponding to all the aspects of LOCATION2.\n","\"\"\"\n","\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety','shopping', 'touristy', 'transit-location']\n","location2_aspects_result_analysis = {}\n","\n","for i in range(12):\n","  location2_aspects_result_analysis[aspects[i]] = [[0 ,0], [0 ,0], [0 ,0]]\n","\n","for i in tqdm(range(df['id'].unique().shape[0]*12, df.shape[0]-12, 12), ncols=80):\n","  for j in range(12):\n","    if df.loc[i+j]['actual'] == df.loc[i+j]['predicted']:\n","      location2_aspects_result_analysis[aspects[j]][int(df.loc[i+j]['actual'])][0] += 1\n","    \n","    location2_aspects_result_analysis[aspects[j]][int(df.loc[i+j]['actual'])][1] += 1"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|████████████████████████████████████████| 387/387 [00:01<00:00, 193.74it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"rEc6Qr612lvO","executionInfo":{"status":"ok","timestamp":1653641293795,"user_tz":-420,"elapsed":311,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["for key in location2_aspects_result_analysis.keys():\n","  df_location_aspect.loc[ii] = ['LOCATION2', f\"{key}\", \n","                                location2_aspects_result_analysis[key][0][0], \n","                                location2_aspects_result_analysis[key][0][1], \n","                                location2_aspects_result_analysis[key][1][0], \n","                                location2_aspects_result_analysis[key][1][1], \n","                                location2_aspects_result_analysis[key][2][0], \n","                                location2_aspects_result_analysis[key][2][1]]\n","  ii += 1"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"qNwb2IaEXQcG","executionInfo":{"status":"ok","timestamp":1653641295022,"user_tz":-420,"elapsed":4,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["df_location_aspect['positive percentage'] = round(df_location_aspect['positive correct']/df_location_aspect['positive total']*100, 2)\n","df_location_aspect['negative percentage'] = round(df_location_aspect['negative correct']/df_location_aspect['negative total']*100, 2)\n","df_location_aspect['none percentage'] = round(df_location_aspect['none correct']/df_location_aspect['none total']*100, 2)\n","\n","df_location_aspect['total percentage'] = round((df_location_aspect['positive correct'] + df_location_aspect['negative correct'] + df_location_aspect['none correct'])/(df_location_aspect['positive total'] + df_location_aspect['negative total'] + df_location_aspect['none total'])*100, 2)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"kA3cm8qMVxHo","colab":{"base_uri":"https://localhost:8080/","height":932},"executionInfo":{"status":"ok","timestamp":1653641297292,"user_tz":-420,"elapsed":19,"user":{"displayName":"project colab","userId":"14559293068416285175"}},"outputId":"8b040fe6-ae3b-46bb-90c0-580c8a931069"},"source":["df_location_aspect"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     location            aspect  positive correct  positive total  \\\n","0   LOCATION1            dining              30.0            30.0   \n","1   LOCATION1           general             297.0           359.0   \n","2   LOCATION1      green-nature              32.0            40.0   \n","3   LOCATION1              live              52.0            63.0   \n","4   LOCATION1     multicultural              28.0            39.0   \n","5   LOCATION1         nightlife              59.0            62.0   \n","6   LOCATION1             price              69.0            81.0   \n","7   LOCATION1             quiet               6.0            14.0   \n","8   LOCATION1            safety              56.0            61.0   \n","9   LOCATION1          shopping              59.0            62.0   \n","10  LOCATION1          touristy              18.0            25.0   \n","11  LOCATION1  transit-location             119.0           151.0   \n","12  LOCATION2            dining               4.0             4.0   \n","13  LOCATION2           general              69.0            87.0   \n","14  LOCATION2      green-nature               4.0             7.0   \n","15  LOCATION2              live              12.0            14.0   \n","16  LOCATION2     multicultural               6.0             8.0   \n","17  LOCATION2         nightlife              11.0            12.0   \n","18  LOCATION2             price              24.0            27.0   \n","19  LOCATION2             quiet               0.0             2.0   \n","20  LOCATION2            safety              11.0            14.0   \n","21  LOCATION2          shopping              14.0            15.0   \n","22  LOCATION2          touristy               3.0             5.0   \n","23  LOCATION2  transit-location              17.0            29.0   \n","\n","    negative correct  negative total  none correct  none total  \\\n","0                0.0             2.0        1450.0      1458.0   \n","1               87.0           113.0         893.0      1018.0   \n","2                0.0             0.0        1440.0      1450.0   \n","3               14.0            23.0        1358.0      1404.0   \n","4                1.0             3.0        1444.0      1448.0   \n","5                1.0             2.0        1406.0      1426.0   \n","6              104.0           116.0        1264.0      1293.0   \n","7               11.0            15.0        1457.0      1461.0   \n","8               53.0            66.0        1333.0      1363.0   \n","9                1.0             1.0        1413.0      1427.0   \n","10               0.0             0.0        1452.0      1465.0   \n","11              20.0            33.0        1266.0      1306.0   \n","12               0.0             0.0         380.0       383.0   \n","13              18.0            26.0         240.0       274.0   \n","14               0.0             0.0         376.0       380.0   \n","15               2.0             4.0         361.0       369.0   \n","16               1.0             1.0         376.0       378.0   \n","17               0.0             0.0         369.0       375.0   \n","18              25.0            27.0         320.0       333.0   \n","19               2.0             5.0         380.0       380.0   \n","20              11.0            17.0         349.0       356.0   \n","21               0.0             0.0         371.0       372.0   \n","22               0.0             0.0         381.0       382.0   \n","23               4.0             8.0         335.0       350.0   \n","\n","    positive percentage  negative percentage  none percentage  \\\n","0                100.00                 0.00            99.45   \n","1                 82.73                76.99            87.72   \n","2                 80.00                  NaN            99.31   \n","3                 82.54                60.87            96.72   \n","4                 71.79                33.33            99.72   \n","5                 95.16                50.00            98.60   \n","6                 85.19                89.66            97.76   \n","7                 42.86                73.33            99.73   \n","8                 91.80                80.30            97.80   \n","9                 95.16               100.00            99.02   \n","10                72.00                  NaN            99.11   \n","11                78.81                60.61            96.94   \n","12               100.00                  NaN            99.22   \n","13                79.31                69.23            87.59   \n","14                57.14                  NaN            98.95   \n","15                85.71                50.00            97.83   \n","16                75.00               100.00            99.47   \n","17                91.67                  NaN            98.40   \n","18                88.89                92.59            96.10   \n","19                 0.00                40.00           100.00   \n","20                78.57                64.71            98.03   \n","21                93.33                  NaN            99.73   \n","22                60.00                  NaN            99.74   \n","23                58.62                50.00            95.71   \n","\n","    total percentage  \n","0              99.33  \n","1              85.70  \n","2              98.79  \n","3              95.57  \n","4              98.86  \n","5              98.39  \n","6              96.44  \n","7              98.93  \n","8              96.78  \n","9              98.86  \n","10             98.66  \n","11             94.30  \n","12             99.22  \n","13             84.50  \n","14             98.19  \n","15             96.90  \n","16             98.97  \n","17             98.19  \n","18             95.35  \n","19             98.71  \n","20             95.87  \n","21             99.48  \n","22             99.22  \n","23             91.99  "],"text/html":["\n","  <div id=\"df-0dee824a-9c5a-449e-a843-ce3ce184f2d4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>location</th>\n","      <th>aspect</th>\n","      <th>positive correct</th>\n","      <th>positive total</th>\n","      <th>negative correct</th>\n","      <th>negative total</th>\n","      <th>none correct</th>\n","      <th>none total</th>\n","      <th>positive percentage</th>\n","      <th>negative percentage</th>\n","      <th>none percentage</th>\n","      <th>total percentage</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>LOCATION1</td>\n","      <td>dining</td>\n","      <td>30.0</td>\n","      <td>30.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1450.0</td>\n","      <td>1458.0</td>\n","      <td>100.00</td>\n","      <td>0.00</td>\n","      <td>99.45</td>\n","      <td>99.33</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>LOCATION1</td>\n","      <td>general</td>\n","      <td>297.0</td>\n","      <td>359.0</td>\n","      <td>87.0</td>\n","      <td>113.0</td>\n","      <td>893.0</td>\n","      <td>1018.0</td>\n","      <td>82.73</td>\n","      <td>76.99</td>\n","      <td>87.72</td>\n","      <td>85.70</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LOCATION1</td>\n","      <td>green-nature</td>\n","      <td>32.0</td>\n","      <td>40.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1440.0</td>\n","      <td>1450.0</td>\n","      <td>80.00</td>\n","      <td>NaN</td>\n","      <td>99.31</td>\n","      <td>98.79</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LOCATION1</td>\n","      <td>live</td>\n","      <td>52.0</td>\n","      <td>63.0</td>\n","      <td>14.0</td>\n","      <td>23.0</td>\n","      <td>1358.0</td>\n","      <td>1404.0</td>\n","      <td>82.54</td>\n","      <td>60.87</td>\n","      <td>96.72</td>\n","      <td>95.57</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LOCATION1</td>\n","      <td>multicultural</td>\n","      <td>28.0</td>\n","      <td>39.0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>1444.0</td>\n","      <td>1448.0</td>\n","      <td>71.79</td>\n","      <td>33.33</td>\n","      <td>99.72</td>\n","      <td>98.86</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>LOCATION1</td>\n","      <td>nightlife</td>\n","      <td>59.0</td>\n","      <td>62.0</td>\n","      <td>1.0</td>\n","      <td>2.0</td>\n","      <td>1406.0</td>\n","      <td>1426.0</td>\n","      <td>95.16</td>\n","      <td>50.00</td>\n","      <td>98.60</td>\n","      <td>98.39</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LOCATION1</td>\n","      <td>price</td>\n","      <td>69.0</td>\n","      <td>81.0</td>\n","      <td>104.0</td>\n","      <td>116.0</td>\n","      <td>1264.0</td>\n","      <td>1293.0</td>\n","      <td>85.19</td>\n","      <td>89.66</td>\n","      <td>97.76</td>\n","      <td>96.44</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>LOCATION1</td>\n","      <td>quiet</td>\n","      <td>6.0</td>\n","      <td>14.0</td>\n","      <td>11.0</td>\n","      <td>15.0</td>\n","      <td>1457.0</td>\n","      <td>1461.0</td>\n","      <td>42.86</td>\n","      <td>73.33</td>\n","      <td>99.73</td>\n","      <td>98.93</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>LOCATION1</td>\n","      <td>safety</td>\n","      <td>56.0</td>\n","      <td>61.0</td>\n","      <td>53.0</td>\n","      <td>66.0</td>\n","      <td>1333.0</td>\n","      <td>1363.0</td>\n","      <td>91.80</td>\n","      <td>80.30</td>\n","      <td>97.80</td>\n","      <td>96.78</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>LOCATION1</td>\n","      <td>shopping</td>\n","      <td>59.0</td>\n","      <td>62.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1413.0</td>\n","      <td>1427.0</td>\n","      <td>95.16</td>\n","      <td>100.00</td>\n","      <td>99.02</td>\n","      <td>98.86</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>LOCATION1</td>\n","      <td>touristy</td>\n","      <td>18.0</td>\n","      <td>25.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1452.0</td>\n","      <td>1465.0</td>\n","      <td>72.00</td>\n","      <td>NaN</td>\n","      <td>99.11</td>\n","      <td>98.66</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>LOCATION1</td>\n","      <td>transit-location</td>\n","      <td>119.0</td>\n","      <td>151.0</td>\n","      <td>20.0</td>\n","      <td>33.0</td>\n","      <td>1266.0</td>\n","      <td>1306.0</td>\n","      <td>78.81</td>\n","      <td>60.61</td>\n","      <td>96.94</td>\n","      <td>94.30</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>LOCATION2</td>\n","      <td>dining</td>\n","      <td>4.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>380.0</td>\n","      <td>383.0</td>\n","      <td>100.00</td>\n","      <td>NaN</td>\n","      <td>99.22</td>\n","      <td>99.22</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>LOCATION2</td>\n","      <td>general</td>\n","      <td>69.0</td>\n","      <td>87.0</td>\n","      <td>18.0</td>\n","      <td>26.0</td>\n","      <td>240.0</td>\n","      <td>274.0</td>\n","      <td>79.31</td>\n","      <td>69.23</td>\n","      <td>87.59</td>\n","      <td>84.50</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>LOCATION2</td>\n","      <td>green-nature</td>\n","      <td>4.0</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>376.0</td>\n","      <td>380.0</td>\n","      <td>57.14</td>\n","      <td>NaN</td>\n","      <td>98.95</td>\n","      <td>98.19</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>LOCATION2</td>\n","      <td>live</td>\n","      <td>12.0</td>\n","      <td>14.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>361.0</td>\n","      <td>369.0</td>\n","      <td>85.71</td>\n","      <td>50.00</td>\n","      <td>97.83</td>\n","      <td>96.90</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>LOCATION2</td>\n","      <td>multicultural</td>\n","      <td>6.0</td>\n","      <td>8.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>376.0</td>\n","      <td>378.0</td>\n","      <td>75.00</td>\n","      <td>100.00</td>\n","      <td>99.47</td>\n","      <td>98.97</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>LOCATION2</td>\n","      <td>nightlife</td>\n","      <td>11.0</td>\n","      <td>12.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>369.0</td>\n","      <td>375.0</td>\n","      <td>91.67</td>\n","      <td>NaN</td>\n","      <td>98.40</td>\n","      <td>98.19</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>LOCATION2</td>\n","      <td>price</td>\n","      <td>24.0</td>\n","      <td>27.0</td>\n","      <td>25.0</td>\n","      <td>27.0</td>\n","      <td>320.0</td>\n","      <td>333.0</td>\n","      <td>88.89</td>\n","      <td>92.59</td>\n","      <td>96.10</td>\n","      <td>95.35</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>LOCATION2</td>\n","      <td>quiet</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>2.0</td>\n","      <td>5.0</td>\n","      <td>380.0</td>\n","      <td>380.0</td>\n","      <td>0.00</td>\n","      <td>40.00</td>\n","      <td>100.00</td>\n","      <td>98.71</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>LOCATION2</td>\n","      <td>safety</td>\n","      <td>11.0</td>\n","      <td>14.0</td>\n","      <td>11.0</td>\n","      <td>17.0</td>\n","      <td>349.0</td>\n","      <td>356.0</td>\n","      <td>78.57</td>\n","      <td>64.71</td>\n","      <td>98.03</td>\n","      <td>95.87</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>LOCATION2</td>\n","      <td>shopping</td>\n","      <td>14.0</td>\n","      <td>15.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>371.0</td>\n","      <td>372.0</td>\n","      <td>93.33</td>\n","      <td>NaN</td>\n","      <td>99.73</td>\n","      <td>99.48</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>LOCATION2</td>\n","      <td>touristy</td>\n","      <td>3.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>381.0</td>\n","      <td>382.0</td>\n","      <td>60.00</td>\n","      <td>NaN</td>\n","      <td>99.74</td>\n","      <td>99.22</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>LOCATION2</td>\n","      <td>transit-location</td>\n","      <td>17.0</td>\n","      <td>29.0</td>\n","      <td>4.0</td>\n","      <td>8.0</td>\n","      <td>335.0</td>\n","      <td>350.0</td>\n","      <td>58.62</td>\n","      <td>50.00</td>\n","      <td>95.71</td>\n","      <td>91.99</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0dee824a-9c5a-449e-a843-ce3ce184f2d4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0dee824a-9c5a-449e-a843-ce3ce184f2d4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0dee824a-9c5a-449e-a843-ce3ce184f2d4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"2m6lnQaklouY"},"source":["# Creating preds.jsonl\n","\n","This section constructs the `preds.jsonl` file which contains model predictions and original annotations in the following json format.\n","\n","\n","```\n","{\n","  \"opinions\": [\n","    {\n","      \"sentiment\": \"Positive\",\n","      \"aspect\": \"safety\",\n","      \"target_entity\": \"LOCATION1\"\n","    }\n","  ],\n","  \"id\": 153,\n","  \"text\": \" LOCATION1 is in Greater London and is a very safe place\",\n","  \"model_pred\": [\n","    {\n","      \"sentiment\": ...,\n","      \"aspect\": ...,\n","      \"target_entity\":...\n","    },...\n","  ]\n","}\n","```"]},{"cell_type":"code","metadata":{"id":"AVfjawsokj_t","executionInfo":{"status":"ok","timestamp":1653641304071,"user_tz":-420,"elapsed":860,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["with open('/content/drive/MyDrive/PIL/BERT-ABSA/SentiHood Dataset/sentihood-test.json', 'r') as fp:\n","  testing_set = json.load(fp)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"uQyXSIuUlyz_","executionInfo":{"status":"ok","timestamp":1653641305580,"user_tz":-420,"elapsed":6,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["labels_to_sentiment_dict = {\n","    0: 'Positive',\n","    1: 'Negative',\n","    2: 'None'\n","}"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"2V00G2TJl4v2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1653642240397,"user_tz":-420,"elapsed":912711,"user":{"displayName":"project colab","userId":"14559293068416285175"}},"outputId":"20d32044-0707-40ad-afd7-dd66996d5093"},"source":["BERT_MODEL = 'bert-base-uncased'\n","MAX_LEN = 160\n","locations = ['LOCATION1', 'LOCATION2']\n","aspects = ['dining', 'general', 'green-nature', 'live', 'multicultural', 'nightlife', 'price', 'quiet', 'safety','shopping', 'touristy', 'transit-location']\n","\n","tokenizer = transformers.BertTokenizer.from_pretrained(BERT_MODEL)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Device: {device}\")\n","\n","model = torch.load('/content/drive/MyDrive/PIL/BERT-ABSA/Bert-pair/NLI-B/Models/Models3.bin')\n","\n","for each_example in tqdm(testing_set, ncols=80):\n","  id = each_example['id']\n","  text = each_example['text'].strip()\n","\n","  each_example['model_pred'] = []\n","\n","  count_location = 1\n","  for location in locations:\n","    if location in text:\n","       # If \"location\" is present in the text, then utilize the trained model\n","      # to predict the aspects and their corresponding sentiment of the text.\n","\n","      text = text.replace(location, 'location - ' + str(count_location))\n","      \n","      for aspect in aspects:\n","        predicted_probabilities = []\n","        aspect_sentiment = None\n","\n","        for polarity in ['Positive', 'Negative', 'None']:\n","          auxiliary_sentence = f\"the polarity of the aspect {aspect} of location - {str(count_location)} is {polarity}.\"\n","          \n","          combined_text = text + ' ' + auxiliary_sentence\n","          \n","          inputs = tokenizer.encode_plus(\n","              combined_text,\n","              add_special_tokens = True,\n","              max_length = MAX_LEN,\n","              pad_to_max_length = True\n","          )\n","          ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).unsqueeze(0)\n","          mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).unsqueeze(0)\n","          token_type_ids = torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long).unsqueeze(0)\n","\n","          ids = ids.to(device, dtype=torch.long)\n","          mask = mask.to(device, dtype=torch.long)\n","          token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","\n","          outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n","          outputs = F.softmax(outputs, dim=-1)\n","\n","          outputs = outputs.detach().cpu().numpy()\n","\n","          predicted_probabilities.append(outputs[0][1])\n","\n","        # Utilizing the matching score to compute the sentiment.\n","        if predicted_probabilities[0] > predicted_probabilities[1] and predicted_probabilities[0] > predicted_probabilities[2]:\n","          aspect_sentiment = 'Positive'\n","        elif predicted_probabilities[1] > predicted_probabilities[0] and predicted_probabilities[1] > predicted_probabilities[2]:\n","          aspect_sentiment = 'Negative'\n","        else:\n","          aspect_sentiment = None\n","\n","        # If predicted sentiment is not None, then add it to the preds.jsonl.\n","        \n","        if aspect_sentiment:\n","          result = {\n","              \"sentiment\": aspect_sentiment,\n","              \"aspect\": aspect,\n","              \"target_entity\": location\n","          }\n","          each_example['model_pred'].append(result)\n","      \n","    count_location += 1"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda:0\n"]},{"output_type":"stream","name":"stderr","text":["  0%|                                                  | 0/1491 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","100%|███████████████████████████████████████| 1491/1491 [15:11<00:00,  1.64it/s]\n"]}]},{"cell_type":"code","metadata":{"id":"LaWQo-rOa6tU","executionInfo":{"status":"ok","timestamp":1653642240682,"user_tz":-420,"elapsed":313,"user":{"displayName":"project colab","userId":"14559293068416285175"}}},"source":["with open('/content/drive/MyDrive/PIL/BERT-ABSA/Bert-pair/NLI-B/preds.jsonl', mode='w', encoding='utf-8') as fp:\n","  for each in testing_set:\n","    json_record = json.dumps(each, ensure_ascii=False)\n","    fp.write(json_record + '\\n')"],"execution_count":24,"outputs":[]}]}